from bpe import BPE
from token_embeddings import TokenEmbeddings
import torch


# üîπ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
bpe = BPE(vocab_size=31)
bpe.fit(
    "–û–¥–Ω–∞–∂–¥—ã –±—ã–ª —Å–ª—É—á–∞–π –≤ –¥–∞–ª—ë–∫–æ–º –ú–∞–∫–∞–æ: –º–∞–∫–∞–∫–∞ –∫–æ–∞–ª—É –≤ –∫–∞–∫–∞–æ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –ª–µ–Ω–∏–≤–æ –∫–∞–∫–∞–æ –ª–∞–∫–∞–ª–∞, –º–∞–∫–∞–∫–∞ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –∏–∫–∞–ª–∞."
)
encoded = bpe.encode(
    "–û–¥–Ω–∞–∂–¥—ã –±—ã–ª —Å–ª—É—á–∞–π –≤ –¥–∞–ª—ë–∫–æ–º –ú–∞–∫–∞–æ: –º–∞–∫–∞–∫–∞ –∫–æ–∞–ª—É –≤ –∫–∞–∫–∞–æ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –ª–µ–Ω–∏–≤–æ –∫–∞–∫–∞–æ –ª–∞–∫–∞–ª–∞, –º–∞–∫–∞–∫–∞ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –∏–∫–∞–ª–∞."
)
print(encoded)

print(bpe.decode(encoded))

bpe.save("data/bpe.dill")
bpe2 = BPE.load("data/bpe.dill")

print(bpe2.tokens)


x = torch.tensor([[113, 456, 76, 345], [345, 678, 454, 546]])
model = TokenEmbeddings(vocab_size=1000, emb_size=64)    
    
print("\n–í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä:")
print(x)

model.forward(x)
