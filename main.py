from bpe import BPE
from token_embeddings import TokenEmbeddings
from positional_embedings import PositionalEmbeddings
import torch


# üîπ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
bpe = BPE(vocab_size=31)
bpe.fit(
    "–û–¥–Ω–∞–∂–¥—ã –±—ã–ª —Å–ª—É—á–∞–π –≤ –¥–∞–ª—ë–∫–æ–º –ú–∞–∫–∞–æ: –º–∞–∫–∞–∫–∞ –∫–æ–∞–ª—É –≤ –∫–∞–∫–∞–æ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –ª–µ–Ω–∏–≤–æ –∫–∞–∫–∞–æ –ª–∞–∫–∞–ª–∞, –º–∞–∫–∞–∫–∞ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –∏–∫–∞–ª–∞."
)
encoded = bpe.encode(
    "–û–¥–Ω–∞–∂–¥—ã –±—ã–ª —Å–ª—É—á–∞–π –≤ –¥–∞–ª—ë–∫–æ–º –ú–∞–∫–∞–æ: –º–∞–∫–∞–∫–∞ –∫–æ–∞–ª—É –≤ –∫–∞–∫–∞–æ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –ª–µ–Ω–∏–≤–æ –∫–∞–∫–∞–æ –ª–∞–∫–∞–ª–∞, –º–∞–∫–∞–∫–∞ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –∏–∫–∞–ª–∞."
)
print(encoded)

print(bpe.decode(encoded))

bpe.save("data/bpe.dill")
bpe2 = BPE.load("data/bpe.dill")

print(bpe2.tokens)


x = torch.tensor([[1, 5, 7, 17], [17, 5, 1, 3]])
model = TokenEmbeddings(vocab_size=20, emb_size=10)    
    
print("\n–í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä:")
print(x)

result = model.forward(x)

print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç:")
print(result)


pos_emb = PositionalEmbeddings(max_seq_len=100, emb_size=64)
result = pos_emb(10)  # shape: (10, 64) - –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫