from bpe import BPE
import torch

device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print("Using:", device)

# –ø—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ MPS
x = torch.randn(1000, 1000, device=device)
y = torch.randn(1000, 1000, device=device)
z = (x @ y).sum()
z.backward() if z.requires_grad else None
print("OK")

# üîπ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
bpe = BPE(vocab_size=31)
bpe.fit(
    "–û–¥–Ω–∞–∂–¥—ã –±—ã–ª —Å–ª—É—á–∞–π –≤ –¥–∞–ª—ë–∫–æ–º –ú–∞–∫–∞–æ: –º–∞–∫–∞–∫–∞ –∫–æ–∞–ª—É –≤ –∫–∞–∫–∞–æ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –ª–µ–Ω–∏–≤–æ –∫–∞–∫–∞–æ –ª–∞–∫–∞–ª–∞, –º–∞–∫–∞–∫–∞ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –∏–∫–∞–ª–∞."
)
encoded = bpe.encode(
    "–û–¥–Ω–∞–∂–¥—ã –±—ã–ª —Å–ª—É—á–∞–π –≤ –¥–∞–ª—ë–∫–æ–º –ú–∞–∫–∞–æ: –º–∞–∫–∞–∫–∞ –∫–æ–∞–ª—É –≤ –∫–∞–∫–∞–æ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –ª–µ–Ω–∏–≤–æ –∫–∞–∫–∞–æ –ª–∞–∫–∞–ª–∞, –º–∞–∫–∞–∫–∞ –º–∞–∫–∞–ª–∞, –∫–æ–∞–ª–∞ –∏–∫–∞–ª–∞."
)
print(encoded)

print(bpe.decode(encoded))

bpe.save('data/bpe.dill')
bpe2 = BPE.load('data/bpe.dill')

print(bpe2.tokens)

# print(bpe.get_token_to_id())
# print(bpe.get_id_to_token())
